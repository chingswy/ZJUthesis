\section{人体二维姿态估计问题的研究}
\subsection{问题描述}
在计算机视觉领域中，人体二维姿态估计是一个极具挑战的任务，人体二维人体姿态估计问题，即是说给定输入的图片或视频序列，进行处理得到相应的人体各个关节在图片上的位置。在深度学习普及之前，该领域的主要研究方法是基于传统的图像处理方法，从图片中提取特征然后进行检测，但是这些传统的处理方法准确率与精度都较低，难以推广到一般场景。自从深度学习方法开始普及之后，对于二维人体姿态估计问题，由于可以由人类标注者在图像中标出人体的关节点的位置，因此可以得到大量的这样的带标注的的训练数据。而基于深度神经网络的方法就可以利用这样大量的数据，对网络进行训练。因此目前的主流的准确率较高的方法都是基于深度学习技术的，本文因此也只介绍这部分内容。

\subsection{研究方法比较}
人体二维姿态估计目前有两个主流方案，第一种是自顶向下的方法，这种方法先检测图片中的人体，得到一个人体的检测框，然后去分别检测每一个人的人体区域中的姿态；另一种是自底向上的方法，首先检测出图片中的所有的人的肢体的关节节点，然后将关节节点进行拼接，得到人体的骨架。自顶向下的方法中，姿态检测的准确度会依赖于人体区域框的检测质量，如果人体区域框的检测不够稳定，那么该方法的输出也会相应的变差。并且如果图片中的人数增加，那么该算法检测到的人体区域的框就会增加，同样的计算时间也会成倍地增加。但是对于自底向上的方法来说，由于是对整张图进行检测，因此计算时间不会随着图片中人数的增加而成倍增加，同时也不会受到前一步检测的人体区域框的质量的影响，检测结果更加鲁棒。但是在自底向上的方法中，如果两个人离得较近，会出现拼接错误的情况；同时由于这种方法依赖的是关节之间的联系，所以对全局信息的获取会有不足。

目前主流的开源人体二维检测方案主要有几种，并且在不同的场景中各有优劣。为了在我们的数据上取得较好的效果，我们对三种方法都进行了研究，并进行了测试，以下依次介绍三种模型。

\subsubsection{OpenPose}
OpenPose\cite{openpose}是由卡耐基梅隆大学的Gines Hidlalgo等人提出的多人二维关键点实时检测方案，该方案是第一个实时的多人二维关键点检测系统，能够同时检测认的身体关键点、手部关键点、脸部关键点及脚部关键点（总共135个关键点）。该方案是基于自底向上的思想的。该方法使用了一种叫部件亲和场(Part Affinity Fields，简称PAF)的非参数表示方法，通过这种方法将检测出来的人体关节组合起来。部件亲和场的表示方法是指，对于人体上的骨架上的每一个像素，去回归这一段骨架的方向向量。如图\ref{fig:2d-op}-(c)所示，对于输入的图片中人体的手臂部分，OpenPose会去回归一个这个手臂上的点沿着人体的关节顺序的指向的方向。例如对于手臂一段，手肘与手腕中间的属于人体的部分的像素，就会输出一个向量表示方向，这些向量都会从手肘指向手腕。该方法具有较高的精度以及较好的实时性。该方法可以同时对人体的身体部分、双脚、手部关节点、脸部关键点进行检测，输出结果较为丰富。
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figure/2dpose/openpose}
    \caption{\label{fig:2d-op} OpenPose流程图}
\end{figure}
该方法主要分为几个步骤，其流程图如图\ref{fig:2d-op}所示。首先对于一张输入的尺寸为\(w\times h\)的图片，如图\ref{fig:2d-op}-a，OpenPose将整张图片输入给前馈神经网络，通过该网络预测一系列的二维位置置信图(confidence map)\(\mathbf{S}\)，置信图表示的人人体的关节的位置，如图\ref{fig:2d-op}-b所示，以及一系列的二维关节亲和向量场(PAFs)\(\mathbf{L}\)，如图\ref{fig:2d-op}-c所示。对于一个人体，如果有\(J\)个关节，那么这一步就会输出\(J\)张置信图，即\(\mathbf{S}=(\mathbf{S}_1, \mathbf{S}_2, ..., \mathbf{S}_J)\)每一张代表了人体的一个关节，其中\(\mathbf{S}_{j}\in \mathbb{R}^{w\times h}$, $j \in \{1\ldots J\}\)。输出的部件亲和场总共有\(C\)个，每一个代表了定义的人体的每一段骨头，即\(\mathbf{L}_{c}\in \mathbb{R}^{w\times h \times 2}$, $c \in \{1\ldots C\}\)，其中对于每一段骨头的部件亲和场表示的是一个二维的向量场，\(\mathbf{L}_{c}\in \mathbb{R}^{w\times h \times 2}, c \in \{1\ldots C\}\)。最后通过对回归的结果进行组合，使用贪心策略匹配属于同一个人的关节，就能得到整个人体的完整的身体姿态。如图\ref{fig:2d-op}-e所示。

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figure/2dpose/openpose-arch}
    \caption{\label{fig:2d-op-net} OpenPose网络结构}
\end{figure}
OpenPose的网络结构如图\ref{fig:2d-op-net}所示。该网络是一个多阶段的卷积神经网络，图中的蓝色部分负责预测部件亲和场，橙色部分负责预测置信图。由于网络是多阶段的，因此上述两个模块可以不断堆叠在一起，并且对每一个阶段都进行监督，相当于对检测结果不断进行优化。网络中的每个卷积模块使用的是三个堆叠在一起的卷积核为\(3\times 3\)的卷积网络。在输出置信图和部件亲和场后需要对不同的人的身体关节进行组合，由于本文研究的场景都是单人的，因此该部分暂时先不考虑。

\subsubsection{AlphaPose}
AlphaPose\cite{AlphaPose}是上海交通大学卢策吾教授团队开源的二维人体关键点检测方法，该方法使用的是自顶向下的方法，即首先检测人体所在的框的区域，再对框中的人体进行关键点检测。
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figure/2dpose/alphapose}
    \caption{\label{fig:2d-ap} AlpahPose流程图}
\end{figure}
该算法流程图如图\ref{fig:2d-ap}所示。首先通过区域候选网络(Region Proposal Network，简写为RPN)提出候选的人体区域，然后将所得到的所有的候选的人体区域框分为两支。图中\ref{fig:2d-ap}上一支通过了空间变换网络(Spatial Transformer Network，简写为STN)，将不规则形状的框转化到统一的空间内进行单人的人体姿态估计(Sigle-Person Pose Estimator，简写为SPPE)，再将结果通过空间反变换网络(Spatial Detransformer Network，简写为SDTN)转化到原始图片坐标系中。另一支直接通过单人姿态估计(Single Person Pose Estimation)算法，进行姿态估计。最后将两支的结果融合，通过姿态非极大值抑制算法得到姿态。

\subsubsection{级联金字塔网络}
级联金字塔网络(Cascaded Pyramid Network, 简称CPN)\cite{CPN}是由旷视科技提出的模型，该模型取得了COCO2017人体关键点挑战赛的冠军，该模型主要面向的问题是单张图片中的多人姿态估计问题。该模型属于自顶向下的方法，即也是首先生成人体的边界框，之后对单个人体的框进行关键点检测。
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figure/2dpose/cpn}
    \caption{\label{fig:2d-cpn} CPN流程图}
\end{figure}
该网络的流程图如图\ref{fig:2d-cpn}所示。级联金字塔网络包含了两个阶段，第一个阶段为全局网络(GlobalNet)，这部分用来从图像中提取特征，可以定位特征较为明显的关键点，例如手和脚，但是无法直接从图片中识别出被遮挡的关键点；第二个阶段为精炼网络(RefineNet)，这一阶段主要通过整合全局网络中识别得到的图片特征，来推断被遮挡住的关键点的位置。即是说理解可以看到的关键点所提供的上下文信息，完成所有关键点的检测任务。

论文中的全局网络的骨架是深度残差网络\cite{resnet}，通过使用U形结构来整合不通尺度的空间分辨率的特征以及语义信息。精炼网络部分中，为了提高计算的效率，同时保持信息在传输过程中的完整性，精炼网络部分首先讲不同尺度的特征图进行堆叠，然后通过上采样和融合，将不同层次的信息进行融合。

\subsection{模型比较}
为了验证上述二维关节点检测模型在我们的实验环境下的准确度，我们需要对其进行定量的分析。
\subsubsection{数据集介绍}
由于我们的实验设备没有在人身上贴标签，因此无法得到准确的三维关键点以及二维关节点的坐标，因此我们选择了广泛使用的Human3.6M数据集\cite{ionescu2014human3}。该数据集是最大的人体三维姿态数据集之一，包含了360万的图片，其中包括了11个演员以及15个日常动作，例如吃饭，走路等。该数据集的采集环境为室内环境，并且使用了总共4个相机来进行数据采集。该数据集通过在人体贴上标志物来获得人体的三维空间的关节位置，相对于从图片直接估计来说较为准确，因此我们通过该数据集来对我们的算法进行定量分析。

\subsubsection{评价标准}
由于我们的任务是进行单人的二维关键点检测，因此不需要考虑多人的二维关键点评价标准。而由这些模型公开的评价指标的数值均是针对多人场景的，因此我们需要对自己得到的结果进行测量。在这里，我们直接使用了在图像中的像素误差值作为真实值与估计值计算的距离，使用的评价标准为平均关节误差，计算公式为
\begin{equation}
    e = \frac{\delta(v_i)\sum_i d_i}{\sum_i\delta(v_i)}
\end{equation}
其中\(d_i\)表示测量的该关节的位置与真实的关节位置的欧式距离，一般写为\(d_i = \sqrt{(\hat{x_i} - x_i)^2 + (\hat{y_i} - y_i)^2}\)，这里的\(v_i\)表示第\(i\)个关节的可见性，如果这个关节在图中可见，那么\(v_i = 1, \delta(v_i) = 1\)，反之，如果该关节不可见，那么\(v_i = 0, \delta(v_i) = 0\)。这里使用\(\delta\)函数是因为一般的模型输出的是一个介于0-1之间的值，表示其可见性的不确定度。
% 在进行人体二维关键点任务的评价时，一般使用的指标是\textbf{关键点相似度}(Object Keypoint Similarity，简称OKS)。因为对于人体来说，如果简单的使用欧氏距离来进行计算的话，相当于没有考虑到人体在图像中的大小会随着距离的改变而改变。因此该指标会考虑到人体的尺度，计算公式为：
% \begin{equation}
%     OKS_p = \frac{\sum_i }{-d^2_}
% \end{equation}

在这一部分中，我们使用Human3.6M数据集的图片，通过三种不同的人体二维关节点检测方法获得其关节位置，与其提供的二维关节点的位置进行比较，计算三种方法的误差。由于二维关节点的训练数据集中，关节的位置通常都是由人类标注者进行标注，而Human3.6M的关节点的位置是由其贴的标志的位置决定的，因此两者的关节定义有所不同，而在三种二维关节点检测方法中，其各自使用的训练数据集也有所区别，因此我们只选择了人的身体上的主要的几个点进行比较。
\begin{figure}[htbp]
    \centering
    \subfigure[原始图片]{
        \begin{minipage}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figure/2dpose/h36m} %
        \end{minipage}% 
    }% 
    \subfigure[OpenPose结果]{
        \begin{minipage}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figure/2dpose/h36mop} %
        \end{minipage}% 
    }%
    \subfigure[CPN结果]{
        \begin{minipage}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figure/2dpose/h36mcp} %
        \end{minipage}% 
    }%  
    \subfigure[AlphaPose结果]{
        \begin{minipage}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figure/2dpose/h36map} %
        \end{minipage}% 
    }%   
    \caption{三种人体二维检测框架在Human3.6M \cite{ionescu2014human}上的结果示例\label{fig:h36mres}}
\end{figure}

\subsubsection{测试结果}
我们取了Human3.6M数据集中的一段视频，该段视频包含了5783帧，时长为1分57秒，视频帧率为50帧每秒。每一个相机拍摄的图片的分辨率为\(1000\times 1002\)，与我们的图片的分辨率接近。每一帧有四个视角的相机，即总共有23132张图片，我们对所有的图片都测试了一遍三种模型，得到其各自的结果，运行环境为CPU为 Intel i7-8700，8G RAM，显卡为GeForce GTX 1060 6GB GPU，系统平台为Ubuntu 16.04。OpenPose的源代码为C++实现，需要对源代码进行编译，得到可执行文件运行；AlphaPose基于PyTorch实现，需要配置PyTorch 1.0版本的环境；CPN是基于TensorFlow实现的代码，配置TensorFlow 1.9.0版本即可运行代码。代码配置过程此处省略不表。

三种模型的二维关键点检测结果如图\ref{fig:h36mres}所示。从可视化的结果来看，三种模型的检测结果基本一致，我们需要进一步的定量计算。在这里计算时，由于网络输出具有不确定性，对每个关节的结果都会由一个范围为\((0,1)\)的置信度，我们这里进行量化的时候只考虑那些置信度大于0.5的关节检测结果。首先计算三种方法的检测的平均关节误差，如表\ref{tab:2derror}所示。从表\ref{tab:2derror}中可以看出，AlphaPose的误差均值最低，并且数据的离散程度也较低。OpenPose方法虽然在多人的场景下表现较好，但是在这种单人背景简单的场景下表现不如自顶向下的两种方法。
\begin{table}[H]
    \centering
    \begin{tabular}{lccc}
        \hline
        Name                    & OpenPose & CPN     & AlphaPose \\
        \hline
        \text{误差均值（像素）} & 9.62  & 9.32 & 8.78   \\
        \text{误差标准差}       & 6.35  & 7.85 & 4.63   \\
        \hline
    \end{tabular}
    \caption{三种二维检测人体方法的误差\label{tab:2derror}}
\end{table}

为了更直观的观察其误差，我分别计算了各个关节的误差的分布，并统计了各个关节误差的直方图，如图\ref{fig:2d-loss}所示。从表\ref{tab:2derrorjoint}中可以看出，对于接近人的躯干的部位，如臀部和肩部，其误差的波动大多比处于人的躯干末端的部位小。从误差的直方图中可以看出，对于躯干上的关节点（躯干只左右肩、左右臀部），其误差分布比较接近正态分布，数据都集中在一块，且极差相对较小。而对于属于手和脚的关节点，如图\ref{fig:2d-loss}的第二、三列所示，会出现极个别的误差特别大的点。经过可视化发现，这些误差大的地方主要是人的左右出现了误匹配的情况，因此其置信度较大，但是实际上是匹配错误的，这也是深度神经网络的缺陷之一。因此我们需要在接下来的工作中考虑如何去除掉这些误匹配的离群点，如何利用多视图的信息来对离群点进行剔除。
\begin{table}[H]
    \centering
    \begin{tabular}{lcccccc}
        \hline
        名称      & 左臀     & 左膝    & 左踝     & 右臀     & 右膝    & 右踝     \\
        \hline
        OpenPose  & 11.5,6.0 & 8.8,6.4 & 15.9,7.6 & 15.9,5.0 & 8.0,7.4 & 11.1,9.8 \\
        CPN       & 11.4,5.9 & 7.7,5.5 & 14.0,7.8 & 16.2,5.1 & 6.9,6.6 & 9.8,8.2  \\
        AlphaPose & 12.2,5.2 & 7.8,4.3 & 13.6,5.7 & 17.2,5.2 & 6.7,5.0 & 9.6,5.6  \\
        \hline
        名称      & 左肩     & 左肘    & 左腕     & 右肩     & 右肘    & 右腕     \\
        \hline
        OpenPose  & 7.5,4.7  & 8.0,5.4 & 5.9,6.6  & 9.0,6.4  & 7.3,5.0 & 6.5,6.0  \\
        CPN       & 7.1,4.2  & 7.2,7.4 & 7.1,15.6 & 8.5,5.9  & 7.0,7.8 & 8.9,14.3 \\
        AlphaPose & 6.8,4.3  & 6.7,4.4 & 5.1,3.3  & 7.9,5.4  & 6.0,3.8 & 5.8,3.6  \\
        \hline
        \end{tabular}
    \caption{三种二维检测人体方法的各关节误差均值,方差（单位：像素）\label{tab:2derrorjoint}}
\end{table}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/2dpose/compare}
    \caption{\label{fig:2d-loss} 各个关节的误差分布对比}
\end{figure}

基于以上比较，我们发现AlphaPose的精度较高，因此我们选择使用该模型在我们的模型上应用。部分图片的二维人体关节点检测结果如图所示。从二维关节点检测结果可以看出，目前的二维人体关节点模型可以在我们的数据上取得较好的结果。基于这一部分的结果，我们才能对人体的三维关节点进行重建。

% \unsure{}{在CMU数据集上也需要测一下，但是他们那个没有2d的GT需要自己投影一下}

\subsection{应用}
在对三种二维检测模型进行比较之后，我们选择了AlphaPose作为我们的二维检测工具，我们采集了三个不同的人的数据，并使用了AlphaPose在数据上进行了应用，得到的部分结果如图\ref{fig:ls2d}所示。图中包含了两个不同的角色的总共四个不同的相机视角。从图中我们可以看出，目前的前沿的二维人体检测方法具有较好的泛化能力，可以在不同的光照条件、人体大小、外观衣着下都能取得好的结果。

\begin{figure}[htbp]
    \centering
    \subfigure[演员冯]{
        \begin{minipage}[t]{0.3\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figure/result/alphapose/vis/feng} %
        \end{minipage}% 
    }% 
    \subfigure[演员帅]{
        \begin{minipage}[t]{0.3\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figure/result/alphapose/vis/shuai} %
        \end{minipage}% 
    }%
    \subfigure[演员柯]{
        \begin{minipage}[t]{0.3\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figure/result/alphapose/vis/ke} %
        \end{minipage}% 
    }%  
    \caption{人体二维检测框架AlphaPose在LightStage数据上的应用结果\label{fig:ls2d}}
\end{figure}

尽管OpenPose在精度上低于AlphaPose，但是该模型能够同时检测图片中的手部关键点、脸部关键点，以及脚部关键点。因此我们同样使用该模型在我们的数据上进行了测试，并对结果进行了可视化。实验环境为Ubuntu 16.04, GPU GeForce GTX 1060 6GB，平均运行时间为2.59帧每秒。

\begin{figure}[htbp]
    \centering
    \subfigure[演员冯]{
        \begin{minipage}[t]{0.3\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figure/result/openpose/feng_rendered} %
        \end{minipage}% 
    }% 
    \subfigure[演员帅]{
        \begin{minipage}[t]{0.3\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figure/result/openpose/shuai_rendered} %
        \end{minipage}% 
    }%
    \subfigure[演员柯]{
        \begin{minipage}[t]{0.3\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figure/result/openpose/ke_rendered} %
        \end{minipage}% 
    }%  
    \caption{人体二维检测框架OpenPose在LightStage数据上的应用结果\label{fig:ls2dop}}
\end{figure}
% \unsure{}{感觉需要一下将图片做个直方图矫正}
从可视化的结果(图\ref{fig:ls2d},图\ref{fig:ls2dop})中我们可以看到，基于深度学习的方法能够在我们的场景中容易地检测出人体的各个关节的位置，我们的演员的服装、动作均可以不受实验环境的限制，可以较为自由地实现各种动作，并且二维检测方法还能检测出这些动作下的关键点位置。同时，由于我们的视频采集得到的是分辨率为\(2048\times 2048\)的高清照片，因此即使在这种有一定距离的场景下，依然能保持脸部的清晰度，也能够实现对人体面部关键点、手部关键点的较为准确的检测。

\subsection{本章小结}
在本章中，我们对三种基于深度学习的人体二维关键点检测方法进行了学习，实现了在我们的数据上对人体二维关键点的检测功能。为了对二维关键点检测进行比较，我们选择了与我们的实验场景类似的人体大型开源数据集Human3.6M，并在该数据集上进行了定量的实验与误差分析，得到了AlphaPose精度更高的结论，为后续的工作提供了第一步的结果。这一部分存在的问题是，基于深度学习的方法在进行人体二维关键点检测时会出现检测错误的情况，因此我们在接下来的工作中会继续探究这一问题。
