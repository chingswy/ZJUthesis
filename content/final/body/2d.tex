\section{人体二维姿态估计问题的研究}
\subsection{问题描述}
人体二维人体姿态估计问题，即是说给定输入的图片或视频序列，进行处理得到相应的人体各个关节在图片上的位置。
\unsure{}{这里找一下二维估计的论文，把他们的研究背景抄一下}

\subsection{研究方法比较}
人体二维姿态估计目前有两个主流方案，第一种是自顶向下的方法，这种方法先检测图片中的人体，得到一个人体的检测框，然后去分别检测每一个人的人体区域中的姿态；另一种是自底向上的方法，首先检测出图片中的所有的人的肢体的关节节点，然后将关节节点进行拼接，得到人体的骨架。自顶向下的方法中，姿态检测的准确度会依赖于人体区域框的检测质量；而在自底向上的方法中，如果两个人离得较近，会出现拼接错误的情况；同时由于依赖的是关节之间的联系，所以对全局信息的获取会有不足。目前主流的开源人体二维检测方案主要有几种，并且在不同的场景中各有优劣。为了在我们的数据上取得较好的效果，我们对三种方法都进行了研究，并进行了测试，以下依次介绍三种模型。

\subsubsection{OpenPose}
OpenPose是由CMU提出的多人二维关键点实时检测方案，该方案是基于自底向上的思想的。该方法使用了一种叫部件亲和场(Part Affinity Fields，简称PAF)的非参数表示方法，通过这种方法将检测出来的人体关节组合起来。部件亲和场的表示方法是指，对于人体上的骨架上的每一个像素，去回归这一段骨架的方向向量。该方法具有较高的精度以及较好的实时性。该方法可以同时对人体的身体部分、双脚、手部关节点、脸部关键点进行检测，输出结果较为丰富。
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figure/2dpose/openpose}
    \caption{\label{fig:2d-op} OpenPose流程图}
\end{figure}
该方法主要分为几个步骤，首先对于一张输入的图片，OpenPose将整张图片都输入给深度卷积神经网络，通过网络同时去回归人体的独立的关节点的位置，以及部件亲和场。通过对回归的结果进行组合将候选的关节进行组合，匹配属于同一个人的关节，最后就能得到整个人体的完整的身体姿态。

\subsubsection{AlphaPose}
AlphaPose是上海交通大学卢策吾教授团队开源的二维人体关键点检测方法，该方法使用的是自顶向下的方法，即首先检测人体所在的框的区域，再对框中的人体进行关键点检测。首先通过区域候选网络(Region Proposal Network)提出候选的人体区域，然后将所得到的所有的候选的人体区域框分为两支，一支通过单人姿态估计(Single Person Pose Estimation)算法，以及姿态非极大值抑制算法得到姿态，另一支直接通过单人姿态估计得到人体的姿态，最后将两支分路的结果进行整合，得到最终的人体姿态。
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figure/2dpose/alphapose}
    \caption{\label{fig:2d-ap} AlpahPose流程图}
\end{figure}

\subsubsection{CPN}
级联金字塔网络(Cascaded Pyramid Network, 简称CPN)是有旷视科技提出的模型，该模型取得了COCO2017人体关键点挑战赛的冠军，该模型主要面向的问题是单张图片中的多人姿态估计问题。该模型属于自顶向下的方法，即首先生成人体的边界框，之后对单个人体的框进行关键点检测。级联金字塔网络包含了两个阶段，第一个阶段为全局网络(GlobalNet)，这部分用来从图像中提取特征，可以定位特征较为明显的关键点，例如手和脚，但是无法直接从图片中识别出被遮挡的关键点；第二个阶段为精炼网络(RefineNet)，这一阶段主要通过整合全局网络中识别得到的图片特征，来推断被遮挡住的关键点的位置。即是说理解可以看到的关键点所提供的上下文信息，完成所有关键点的检测任务。
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figure/2dpose/cpn}
    \caption{\label{fig:2d-cpn} CPN流程图}
\end{figure}
论文中的全局网络的骨架是深度残差网络\cite{resnet}，通过使用U形结构来整合不通尺度的空间分辨率的特征以及语义信息。精炼网络部分中，为了提高计算的效率，同时保持信息在传输过程中的完整性，精炼网络部分首先讲不同尺度的特征图进行堆叠，然后通过上采样和融合，将不同层次的信息进行融合。

\subsection{模型比较}
为了验证上述二维关节点检测模型在我们的实验环境下的准确度，我们需要对其进行定量的分析。由于我们的实验设备没有在人身上贴标签，因此无法得到准确的三维关键点以及二维关节点的坐标，因此我们选择了广泛使用的Human3.6M数据集。该数据集是最大的人体三维姿态数据集之一，包含了360万的图片，其中包括了11个演员以及15个日常动作，例如吃饭，走路等。该数据集的采集环境为室内环境，并且使用了总共4个相机来进行数据采集。该数据集通过在人体贴上标志物来获得人体的三维空间的关节位置，相对于从图片直接估计来说较为准确，因此我们通过该数据集来对我们的算法进行定量分析。

在这一部分中，我们使用Human3.6M数据集的图片，通过三种不同的人体二维关节点检测方法获得其关节位置，与其提供的二维关节点的位置进行比较，计算三种方法的误差。由于二维关节点的训练数据集中，关节的位置通常都是由人类标注者进行标注，而Human3.6M的关节点的位置是由其贴的标志的位置决定的，因此两者的关节定义有所不同，而在三种二维关节点检测方法中，其各自使用的训练数据集也有所区别，因此我们只选择了人的身体上的主要的几个点进行比较。三种模型的检测结果如图\ref{fig:h36mres}所示。
\begin{figure}[htbp]
    \centering
    \subfigure[原始图片]{
        \begin{minipage}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figure/2dpose/h36m} %
        \end{minipage}% 
    }% 
    \subfigure[OpenPose结果]{
        \begin{minipage}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figure/2dpose/h36mop} %
        \end{minipage}% 
    }%
    \subfigure[CPN结果]{
        \begin{minipage}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figure/2dpose/h36mcp} %
        \end{minipage}% 
    }%  
    \subfigure[AlphaPose结果]{
        \begin{minipage}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figure/2dpose/h36map} %
        \end{minipage}% 
    }%   
    \caption{三种人体二维检测框架在Human3.6M \cite{ionescu2014human}上的结果示例\label{fig:h36mres}}
\end{figure}
从可视化的结果来看，三种模型的检测结果基本一致，因此我们需要进一步的定量计算。我们取了Human3.6M数据集中的其中一段视频，该段视频包含了5783帧，每一帧有四个视角的相机，即总共有23132张图片，我们对所有的图片都运行了一遍三种模型，得到其各自的结果，运行环境为Intel i7-8700 CPU，8G RAM，以及GeForce GTX 1060 6GB GPU，系统平台为Ubuntu 16.04。
\begin{table}[H]
    \centering
    \begin{tabular}{lccc}
        \hline
        Name                    & OpenPose & CPN     & AlphaPose \\
        \hline
        \text{误差均值（像素）} & 9.62135  & 9.32769 & 8.78416   \\
        \text{误差标准差}       & 6.35488  & 7.85625 & 4.63783   \\
        \hline
    \end{tabular}
    \caption{三种二维检测人体方法的误差\label{tab:2derror}}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{lcccccc}
        \hline
         名称      & 左臀     & 左膝    & 左踝     & 右臀     & 右膝    & 右踝     \\
        \hline
         OpenPose  & 11.5±6.0 & 8.8±6.4 & 15.9±7.6 & 15.9±5.0 & 8.0±7.4 & 11.1±9.8 \\
         CPN       & 11.4±5.9 & 7.7±5.5 & 14.0±7.8 & 16.2±5.1 & 6.9±6.6 & 9.8±8.2  \\
         AlphaPose & 12.2±5.2 & 7.8±4.3 & 13.6±5.7 & 17.2±5.2 & 6.7±5.0 & 9.6±5.6  \\
        \hline
         名称      & 左肩     & 左肘    & 左腕     & 右肩     & 右肘    & 右腕     \\
        \hline
         OpenPose  & 7.5±4.7  & 8.0±5.4 & 5.9±6.6  & 9.0±6.4  & 7.3±5.0 & 6.5±6.0  \\
         CPN       & 7.1±4.2  & 7.2±7.4 & 7.1±15.6 & 8.5±5.9  & 7.0±7.8 & 8.9±14.3 \\
         AlphaPose & 6.8±4.3  & 6.7±4.4 & 5.1±3.3  & 7.9±5.4  & 6.0±3.8 & 5.8±3.6  \\
        \hline
        \end{tabular}
    \caption{三种二维检测人体方法的各关节误差（单位：像素）\label{tab:2derrorjoint}}
\end{table}
从表\ref{tab:2derror}中可以看出，AlphaPose的误差均值最低，并且数据的离散程度也较低，为了更直观的观察其误差，我分别计算了各个关节的误差的分布（只保留置信度大于0.5的检测结果），并统计了各个关节误差的直方图，如图\ref{fig:2d-loss}所示。从图中可以看出，对于躯干上的关节点（躯干只左右肩、左右臀部），其误差分布比较接近正态分布，数据都集中在一块，且最大值不会特别大。而对于属于手和脚的关节点，如图\ref{fig:2d-loss}的第二、三列所示，会出现极个别的误差特别大的点。经过可视化发现，这些误差大的地方主要是人的左右出现了误匹配的情况，因此其置信度较大，但是实际上是匹配错误的，这也是深度神经网络的缺陷之一。因此我们需要在接下来的工作中考虑如何去除掉这些误匹配的离群点。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figure/2dpose/compare}
    \caption{\label{fig:2d-loss} 各个关节的误差分布对比}
\end{figure}

基于以上比较，我们发现AlphaPose的精度较高，因此我们选择使用该模型在我们的模型上应用。部分图片的二维人体关节点检测结果如图所示。从二维关节点检测结果可以看出，目前的二维人体关节点模型可以在我们的数据上取得较好的结果。基于这一部分的结果，我们才能对人体的三维关节点进行重建。

\unsure{}{在CMU数据集上也需要测一下，但是他们那个没有2d的GT需要自己投影一下}

\subsection{应用}
在对三种二维检测模型进行比较之后，我们选择了AlphaPose作为我们的二维检测工具，我们采集了三个不同的人的数据，并使用了AlphaPose在数据上进行了应用，得到的部分结果如图\ref{fig:ls2d}所示。图中包含了两个不同的角色的总共四个不同的相机视角。从图中我们可以看出，目前的前沿的二维人体检测方法具有较好的泛化能力，可以在不同的光照条件、人体大小、外观衣着下都能取得好的结果。

\begin{figure}[htbp]
    \centering
    \subfigure[演员1相机10]{
        \begin{minipage}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figure/2dpose/231_10_0} %
        \end{minipage}% 
    }% 
    \subfigure[演员1相机23]{
        \begin{minipage}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figure/2dpose/231_23_0} %
        \end{minipage}% 
    }%
    \subfigure[演员2相机4]{
        \begin{minipage}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figure/2dpose/233_4_0} %
        \end{minipage}% 
    }%  
    \subfigure[演员2相机14]{
        \begin{minipage}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figure/2dpose/233_14_0} %
        \end{minipage}% 
    }%   
    \caption{人体二维检测框架在LightStage数据上的应用结果\label{fig:ls2d}}
\end{figure}
